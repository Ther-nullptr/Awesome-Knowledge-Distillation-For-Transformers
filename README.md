# Awesome-Knowledge-Distillation-For-Transformers

Papers for knowledge distillation for NLP and ASR (mainly focus on BERT-like models).

ðŸŒŸ represents important papers.

ðŸ“• represents NLP.

ðŸŽµ represents ASR.

## 2019

* ðŸ“• PKD: [Patient Knowledge Distillation for BERT Model Compression](https://arxiv.org/abs/1908.09355)

* ðŸŒŸðŸ“• DistilBERT: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)



## 2020

* ðŸŒŸðŸ“• TinyBERT: [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
