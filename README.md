# Awesome-Knowledge-Distillation-For-Transformers

Papers for knowledge distillation for NLP and ASR (mainly focus on BERT-like models).

ðŸŒŸ represents important papers.

ðŸ“• represents NLP.

ðŸŽµ represents ASR.

## basic papers

* **Response-Based Knowledge** [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

* **Feature-Based Knowledge** [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550)

* **Relation-Based Knowledge** [A gift from knowledge distillation: Fast optimization, network minimization and transfer learning](https://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html)

## 2019

* ðŸ“• PKD: [Patient Knowledge Distillation for BERT Model Compression](https://arxiv.org/abs/1908.09355)

* ðŸŒŸðŸ“• DistilBERT: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)

* ðŸŒŸðŸ“• TinyBERT: [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)

* ðŸ“• IR-KD: [Knowledge Distillation from Internal Representations](https://arxiv.org/abs/1910.03723v2)

## 2020

* ðŸ“• MobileBERT: [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)

* ðŸ“• CKD: [Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers](https://arxiv.org/abs/2010.03034)

* ðŸ“• ALP-KD: [ALP-KD: Attention-Based Layer Projection for Knowledge Distillation](https://arxiv.org/abs/2012.14022)

* ðŸ“• Co-DIR: [Contrastive Distillation on Intermediate Representations for Language Model Compression](https://arxiv.org/abs/2009.14167)


## 2021

* ðŸŒŸðŸŽµ DistilHubert: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https://arxiv.org/abs/2110.01900)

* ðŸ“• RAIL-KD: [RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation](https://arxiv.org/abs/2109.10164)

## 2022

* ðŸ“• CoFi: [Structured Pruning Learns Compact and Accurate Models](https://arxiv.org/abs/2204.00408)

* ðŸŽµ FitHubert: [FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Learning](https://arxiv.org/abs/2207.00555)

* ðŸŽµ LightHubert: [LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT](https://arxiv.org/abs/2203.15610)
