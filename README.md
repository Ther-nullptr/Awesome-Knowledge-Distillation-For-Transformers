# Awesome-Knowledge-Distillation-For-Transformers

Papers for knowledge distillation for NLP and ASR (mainly focus on BERT-like models).

ðŸŒŸ represents important papers.

ðŸ“• represents NLP.

ðŸŽµ represents ASR.

## 2019

* ðŸ“• PKD: [Patient Knowledge Distillation for BERT Model Compression](https://arxiv.org/abs/1908.09355)

* ðŸŒŸðŸ“• DistilBERT: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)

* ðŸŒŸðŸ“• TinyBERT: [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)

## 2020



* ðŸ“• MobileBERT: [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)

* ðŸ“• CKD: [Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers](https://arxiv.org/abs/2010.03034)

* ðŸ“• ALP-KD: [ALP-KD: Attention-Based Layer Projection for Knowledge Distillation](https://arxiv.org/abs/2012.14022)

## 2021

* ðŸŒŸðŸŽµ DistilHubert: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https://arxiv.org/abs/2110.01900)

## 2022

* ðŸŽµ FitHubert: [FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Learning](https://arxiv.org/abs/2207.00555)

* ðŸŽµ LightHubert: [LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT](https://arxiv.org/abs/2203.15610)
